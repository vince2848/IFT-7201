{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne import Model\n",
    "from copy import deepcopy  # NEW\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.__buffer_size = buffer_size\n",
    "        # TODO : Add any needed attributes\n",
    "        self.data = deque(maxlen=int(buffer_size))\n",
    "\n",
    "    def store(self, element):\n",
    "        \"\"\"\n",
    "        Stores an element. If the replay buffer is already full, deletes the oldest\n",
    "        element to make space.\n",
    "        \"\"\"\n",
    "        # TODO : Implement\n",
    "        self.data.append(element)\n",
    "        pass\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns a list of batch_size elements from the buffer.\n",
    "        \"\"\"\n",
    "        buffer_list = list(self.data)\n",
    "        return random.sample(buffer_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Model):\n",
    "    def __init__(self, actions, *args, **kwargs):\n",
    "        self.actions = actions\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Returns the selected action according to an epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        if bernoulli.rvs(epsilon):\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q_vals = self.predict(state)\n",
    "            action = self.actions[np.argmax(q_vals)]\n",
    "        return action\n",
    "\n",
    "\n",
    "    def soft_update(self, other, tau):\n",
    "        \"\"\"\n",
    "        Code for the soft update between a target network (self) and\n",
    "        a source network (other).\n",
    "\n",
    "        The weights are updated according to the rule in the assignment.\n",
    "        \"\"\"\n",
    "        new_weights = {}\n",
    "\n",
    "        own_weights = self.get_weight_copies()\n",
    "        other_weights = other.get_weight_copies()\n",
    "\n",
    "        for k in own_weights:\n",
    "            new_weights[k] = (1 - tau) * own_weights[k] + tau * other_weights[k]\n",
    "\n",
    "        self.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with 3 hidden layers of hidden dimension 64.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, n_hidden_layers=3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        layers = [torch.nn.Linear(in_dim, hidden_dim), torch.nn.ReLU()]\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.extend([torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU()])\n",
    "        layers.append(torch.nn.Linear(hidden_dim, out_dim))\n",
    "\n",
    "        self.fa = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fa(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch(batch, target_network, gamma):\n",
    "    \"\"\"\n",
    "    Input : \n",
    "        - batch, a list of n=batch_size elements from the replay buffer\n",
    "        - target_network, the target network to compute the one-step lookahead target\n",
    "        - gamma, the discount factor\n",
    "\n",
    "    Returns :\n",
    "        - states, a numpy array of size (batch_size, state_dim) containing the states in the batch\n",
    "        - (actions, targets) : where actions and targets both\n",
    "                      have the shape (batch_size, ). Actions are the \n",
    "                      selected actions according to the target network\n",
    "                      and targets are the one-step lookahead targets.\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    states=np.array([x[0] for x in batch])\n",
    "    \n",
    "    actions_taken=np.array([x[1] for x in batch])\n",
    "    \n",
    "    rewards=np.array([x[2] for x in batch])\n",
    "    \n",
    "    next_states=np.array([x[3] for x in batch])\n",
    "    \n",
    "    terminal=np.array([x[4] for x in batch])\n",
    "    \n",
    "    \n",
    "    #actions = np.array([target_network.get_action(state, 0) for state in states])\n",
    "    \n",
    "    next_qvals_predicted = target_network.predict(next_states)\n",
    "    \n",
    "    next_actions_vals_selected = np.max(next_qvals_predicted,axis=1)\n",
    "    \n",
    "    targets = rewards + gamma*next_actions_vals_selected*(1-terminal)\n",
    "    \n",
    "    #print(type(actions_taken[0]))\n",
    "    \n",
    "    act_targ = (actions_taken.astype(np.int64), targets.astype(np.float32))\n",
    "    \n",
    "    return states, act_targ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_loss(y_pred, y_target):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - y_pred, (batch_size, n_actions) Tensor outputted by the network\n",
    "        - y_target = (actions, targets), where actions and targets both\n",
    "                      have the shape (batch_size, ). Actions are the \n",
    "                      selected actions according to the target network\n",
    "                      and targets are the one-step lookahead targets.\n",
    "\n",
    "    Returns :\n",
    "        - The DQN loss \n",
    "    \"\"\"\n",
    "    # C'est essentiellement le même travail que ce qui est fait dans update_theta\n",
    "    # sauf (1) qu'on le fait en PyTorch et les fonctions n'ont pas le même nom\n",
    "    # et (2) on ne calcule pas le gradient nous-mêmes, on fait juste donner la perte.\n",
    "    # C'est PyTorch qui fait la descente de gradient pour nous.\n",
    "    #actions, q_target = y_target\n",
    "    \n",
    "    #q_predicted = y_pred.gather(1, actions.unsqueeze(-1)).squeeze()\n",
    "    \n",
    "    #return torch.nn.functional.mse_loss(q_predicted, q_target)\n",
    "    #########\n",
    "    \n",
    "    actions, targets = y_target\n",
    "    #print(actions)\n",
    "    #print(targets)\n",
    "    #print()\n",
    "    \n",
    "    q_pred = y_pred.gather(1, actions.unsqueeze(-1)).squeeze()\n",
    "\n",
    "    return torch.nn.functional.mse_loss(q_pred, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(environment, seed):\n",
    "    environment.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)  # NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW : Added lr argument\n",
    "def main(batch_size, gamma, buffer_size, seed, tau, training_interval, lr):\n",
    "    environment = gym.make(\"LunarLander-v2\")\n",
    "    set_random_seed(environment, seed)\n",
    "\n",
    "    actions = list(range(environment.action_space.n))\n",
    "    model = NNModel(environment.observation_space.shape[0], environment.action_space.n)\n",
    "    policy_net = DQN(\n",
    "        actions,\n",
    "        model,\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=lr),\n",
    "        loss_function=dqn_loss,\n",
    "    )\n",
    "    # NEW: pass a deep copy of the model\n",
    "    target_net = DQN(actions, deepcopy(model), optimizer=\"sgd\", loss_function=dqn_loss,)\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    training_done = False\n",
    "    episodes_done = 0\n",
    "    steps_done = 0\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.99\n",
    "    epsilon_min = 0.01\n",
    "\n",
    "    while not training_done:\n",
    "        s = environment.reset()\n",
    "        \n",
    "        episode_done = False\n",
    "        while not episode_done:\n",
    "            a = policy_net.get_action(s, epsilon)\n",
    "            next_s, r, episode_done, _ = environment.step(a)\n",
    "            replay_buffer.store((s, a, r, next_s, episode_done))\n",
    "            s = next_s\n",
    "            \n",
    "            steps_done += 1\n",
    "\n",
    "            if steps_done % training_interval == 0:\n",
    "                if len(replay_buffer.data) >= batch_size:\n",
    "                    batch = replay_buffer.get_batch(batch_size)\n",
    "                    x, y = format_batch(batch, target_net, gamma)\n",
    "                    loss = policy_net.train_on_batch(x, y)\n",
    "                    target_net.soft_update(policy_net, tau)\n",
    "\n",
    "        # TODO: update epsilon\n",
    "        epsilon = max(epsilon*epsilon_decay, epsilon_min)\n",
    "        \n",
    "        episodes_done += 1\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    All hyperparameter values and overall code structure are\n",
    "    only given as a baseline. \n",
    "    \n",
    "    You can use them if they help  you, but feel free to implement\n",
    "    from scratch the required algorithms if you wish !\n",
    "    \"\"\"\n",
    "    batch_size = 32\n",
    "    gamma = 0.99\n",
    "    buffer_size = 1e5\n",
    "    seed = 42\n",
    "    tau = 1e-2\n",
    "    training_interval = 4\n",
    "    lr = 5e-4  # NEW lr as parameter\n",
    "\n",
    "    # NEW : pass lr to main()\n",
    "    agent=main(batch_size, gamma, buffer_size, seed, tau, training_interval, lr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(agent):\n",
    "    environment = gym.make(\"LunarLander-v2\")\n",
    "    set_random_seed(environment, seed=42)\n",
    "    \n",
    "    env = gym.wrappers.Monitor(environment, \"demo\", force=True)\n",
    "    \n",
    "    done = False\n",
    "    s = environment.reset().astype(np.float32)\n",
    "    while not done:\n",
    "        env.render()\n",
    "        q_vals = agent.predict(s)\n",
    "        action = np.argmax(q_vals)\n",
    "        next_s, r, done, _ = environment.step(action)\n",
    "        \n",
    "        s = next_s.astype(np.float32)\n",
    "    env.close()\n",
    "run_dqn(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
