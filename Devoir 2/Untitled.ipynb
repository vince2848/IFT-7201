{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne import Model\n",
    "from copy import deepcopy  # NEW\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.__buffer_size = buffer_size\n",
    "        # TODO : Add any needed attributes\n",
    "        self.data = deque(maxlen=int(buffer_size))\n",
    "\n",
    "    def store(self, element):\n",
    "        \"\"\"\n",
    "        Stores an element. If the replay buffer is already full, deletes the oldest\n",
    "        element to make space.\n",
    "        \"\"\"\n",
    "        # TODO : Implement\n",
    "        self.data.append(element)\n",
    "        pass\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns a list of batch_size elements from the buffer.\n",
    "        \"\"\"\n",
    "        buffer_list = list(self.data)\n",
    "        return random.sample(buffer_list, batch_size)\n",
    "\n",
    "\n",
    "class DQN(Model):\n",
    "    def __init__(self, actions, *args, **kwargs):\n",
    "        self.actions = actions\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Returns the selected action according to an epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        if bernoulli.rvs(epsilon):\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            pred_r = self.predict(state)\n",
    "            action = self.actions[np.argmax(pred_r)]\n",
    "        return action\n",
    "\n",
    "\n",
    "    def soft_update(self, other, tau):\n",
    "        \"\"\"\n",
    "        Code for the soft update between a target network (self) and\n",
    "        a source network (other).\n",
    "\n",
    "        The weights are updated according to the rule in the assignment.\n",
    "        \"\"\"\n",
    "        new_weights = {}\n",
    "\n",
    "        own_weights = self.get_weight_copies()\n",
    "        other_weights = other.get_weight_copies()\n",
    "\n",
    "        for k in own_weights:\n",
    "            new_weights[k] = (1 - tau) * own_weights[k] + tau * other_weights[k]\n",
    "\n",
    "        self.set_weights(new_weights)\n",
    "\n",
    "\n",
    "class NNModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with 3 hidden layers of hidden dimension 64.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, n_hidden_layers=3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        layers = [torch.nn.Linear(in_dim, hidden_dim), torch.nn.ReLU()]\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.extend([torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU()])\n",
    "        layers.append(torch.nn.Linear(hidden_dim, out_dim))\n",
    "\n",
    "        self.fa = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fa(x)\n",
    "\n",
    "\n",
    "def format_batch(batch, target_network, gamma):\n",
    "    \"\"\"\n",
    "    Input : \n",
    "        - batch, a list of n=batch_size elements from the replay buffer\n",
    "        - target_network, the target network to compute the one-step lookahead target\n",
    "        - gamma, the discount factor\n",
    "\n",
    "    Returns :\n",
    "        - states, a numpy array of size (batch_size, state_dim) containing the states in the batch\n",
    "        - (actions, targets) : where actions and targets both\n",
    "                      have the shape (batch_size, ). Actions are the \n",
    "                      selected actions according to the target network\n",
    "                      and targets are the one-step lookahead targets.\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    states = np.array([x[0] for x in batch])\n",
    "    actions_taken = np.array([x[1] for x in batch])\n",
    "    rewards = np.array([x[2] for x in batch])\n",
    "    next_states = np.array([x[3] for x in batch])\n",
    "    terminal = np.array([x[4] for x in batch])\n",
    "\n",
    "    next_qvals_predicted = target_network.predict(next_states)\n",
    "\n",
    "    next_actions_vals_selected = np.max(next_qvals_predicted, axis=1)\n",
    "\n",
    "    targets = rewards + gamma * next_actions_vals_selected * (1 - terminal)\n",
    "\n",
    "    act_targ = (actions_taken.astype(np.int64), targets.astype(np.float32))\n",
    "\n",
    "    return states, act_targ\n",
    "\n",
    "\n",
    "def dqn_loss(y_pred, y_target):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - y_pred, (batch_size, n_actions) Tensor outputted by the network\n",
    "        - y_target = (actions, targets), where actions and targets both\n",
    "                      have the shape (batch_size, ). Actions are the \n",
    "                      selected actions according to the target network\n",
    "                      and targets are the one-step lookahead targets.\n",
    "\n",
    "    Returns :\n",
    "        - The DQN loss \n",
    "    \"\"\"\n",
    "    # C'est essentiellement le même travail que ce qui est fait dans update_theta\n",
    "    # sauf (1) qu'on le fait en PyTorch et les fonctions n'ont pas le même nom\n",
    "    # et (2) on ne calcule pas le gradient nous-mêmes, on fait juste donner la perte.\n",
    "    # C'est PyTorch qui fait la descente de gradient pour nous.\n",
    "\n",
    "    actions, targets = y_target\n",
    "    q_pred = y_pred.gather(1, actions.unsqueeze(-1)).squeeze()\n",
    "\n",
    "    return torch.nn.functional.mse_loss(q_pred, targets)\n",
    "\n",
    "def set_random_seed(environment, seed):\n",
    "    environment.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)  # NEW\n",
    "\n",
    "\n",
    "def run_dqn(agent):\n",
    "    environment = gym.make(\"LunarLander-v2\")\n",
    "    set_random_seed(environment, seed=42)\n",
    "\n",
    "    env = gym.wrappers.Monitor(environment, \"demo\", force=True)\n",
    "\n",
    "    done = False\n",
    "    s = environment.reset().astype(np.float32)\n",
    "    while not done:\n",
    "        env.render()\n",
    "        q_vals = agent.predict(s)\n",
    "        action = np.argmax(q_vals)\n",
    "        next_s, r, done, _ = environment.step(action)\n",
    "\n",
    "        s = next_s.astype(np.float32)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# NEW : Added lr argument\n",
    "def main(batch_size, gamma, buffer_size, seed, tau, training_interval, lr):\n",
    "    environment = gym.make(\"LunarLander-v2\")\n",
    "    set_random_seed(environment, seed)\n",
    "\n",
    "    actions = list(range(environment.action_space.n))\n",
    "    model = NNModel(environment.observation_space.shape[0], environment.action_space.n)\n",
    "    policy_net = DQN(\n",
    "        actions,\n",
    "        model,\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=lr),\n",
    "        loss_function=dqn_loss,\n",
    "    )\n",
    "    # NEW: pass a deep copy of the model\n",
    "    target_net = DQN(actions, deepcopy(model), optimizer=\"sgd\", loss_function=dqn_loss,)\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    training_done = False\n",
    "    episodes_done = 0\n",
    "    steps_done = 0\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.9\n",
    "    epsilon_min = 0.01\n",
    "\n",
    "    while not training_done:\n",
    "        s = environment.reset()\n",
    "        episode_done = False\n",
    "        G = 0\n",
    "        while not episode_done:\n",
    "            a = policy_net.get_action(s, epsilon)\n",
    "            next_s, r, episode_done, _ = environment.step(a)\n",
    "            replay_buffer.store((s, a, r, next_s, episode_done))\n",
    "            s = next_s\n",
    "            steps_done += 1\n",
    "            G += r\n",
    "\n",
    "            if steps_done % training_interval == 0:\n",
    "                if len(replay_buffer.data) >= batch_size:\n",
    "                    batch = replay_buffer.get_batch(batch_size)\n",
    "                    x, y = format_batch(batch, target_net, gamma)\n",
    "                    loss = policy_net.train_on_batch(x, y)\n",
    "                    target_net.soft_update(policy_net, tau)\n",
    "\n",
    "        # TODO: update epsilon\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        if G > 200:\n",
    "            training_done = True\n",
    "\n",
    "        episodes_done += 1\n",
    "\n",
    "        if (episodes_done + 1) % 10 == 0:\n",
    "            print(f\"After {episodes_done + 1} trajectoires, we have G_0 = {G:.2f}, {epsilon:4f}\")\n",
    "\n",
    "    return policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 trajectoires, we have G_0 = -500.47, 0.387420\n",
      "After 20 trajectoires, we have G_0 = -694.86, 0.135085\n",
      "After 30 trajectoires, we have G_0 = -117.72, 0.047101\n",
      "After 40 trajectoires, we have G_0 = -415.01, 0.016423\n",
      "After 50 trajectoires, we have G_0 = -178.77, 0.010000\n",
      "After 60 trajectoires, we have G_0 = -188.69, 0.010000\n",
      "After 70 trajectoires, we have G_0 = -128.48, 0.010000\n",
      "After 80 trajectoires, we have G_0 = -147.89, 0.010000\n",
      "After 90 trajectoires, we have G_0 = -128.96, 0.010000\n",
      "After 100 trajectoires, we have G_0 = -267.53, 0.010000\n",
      "After 110 trajectoires, we have G_0 = -132.29, 0.010000\n",
      "After 120 trajectoires, we have G_0 = -102.92, 0.010000\n",
      "After 130 trajectoires, we have G_0 = -176.73, 0.010000\n",
      "After 140 trajectoires, we have G_0 = -122.48, 0.010000\n",
      "After 150 trajectoires, we have G_0 = -118.18, 0.010000\n",
      "After 160 trajectoires, we have G_0 = -155.73, 0.010000\n",
      "After 170 trajectoires, we have G_0 = -179.26, 0.010000\n",
      "After 180 trajectoires, we have G_0 = -193.43, 0.010000\n",
      "After 190 trajectoires, we have G_0 = -270.79, 0.010000\n",
      "After 200 trajectoires, we have G_0 = -179.15, 0.010000\n",
      "After 210 trajectoires, we have G_0 = -315.98, 0.010000\n",
      "After 220 trajectoires, we have G_0 = -99.61, 0.010000\n",
      "After 230 trajectoires, we have G_0 = -171.66, 0.010000\n",
      "After 240 trajectoires, we have G_0 = -214.64, 0.010000\n",
      "After 250 trajectoires, we have G_0 = -57.28, 0.010000\n",
      "After 260 trajectoires, we have G_0 = -73.72, 0.010000\n",
      "After 270 trajectoires, we have G_0 = -100.45, 0.010000\n",
      "After 280 trajectoires, we have G_0 = -98.46, 0.010000\n",
      "After 290 trajectoires, we have G_0 = -88.40, 0.010000\n",
      "After 300 trajectoires, we have G_0 = -49.55, 0.010000\n",
      "After 310 trajectoires, we have G_0 = -24.70, 0.010000\n",
      "After 320 trajectoires, we have G_0 = -63.61, 0.010000\n",
      "After 330 trajectoires, we have G_0 = -34.66, 0.010000\n",
      "After 340 trajectoires, we have G_0 = -38.34, 0.010000\n",
      "After 350 trajectoires, we have G_0 = -19.91, 0.010000\n",
      "After 360 trajectoires, we have G_0 = 6.06, 0.010000\n",
      "After 370 trajectoires, we have G_0 = -53.84, 0.010000\n",
      "After 380 trajectoires, we have G_0 = -53.05, 0.010000\n",
      "After 390 trajectoires, we have G_0 = -57.38, 0.010000\n",
      "After 400 trajectoires, we have G_0 = -59.50, 0.010000\n",
      "After 410 trajectoires, we have G_0 = -3.32, 0.010000\n",
      "After 420 trajectoires, we have G_0 = -15.14, 0.010000\n",
      "After 430 trajectoires, we have G_0 = -75.56, 0.010000\n",
      "After 440 trajectoires, we have G_0 = -42.09, 0.010000\n",
      "After 450 trajectoires, we have G_0 = -47.05, 0.010000\n",
      "After 460 trajectoires, we have G_0 = -29.10, 0.010000\n",
      "After 470 trajectoires, we have G_0 = -37.88, 0.010000\n",
      "After 480 trajectoires, we have G_0 = -79.60, 0.010000\n",
      "After 490 trajectoires, we have G_0 = -46.47, 0.010000\n",
      "After 500 trajectoires, we have G_0 = -50.67, 0.010000\n",
      "After 510 trajectoires, we have G_0 = 26.09, 0.010000\n",
      "After 520 trajectoires, we have G_0 = 197.09, 0.010000\n",
      "After 530 trajectoires, we have G_0 = 1.45, 0.010000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "buffer_size = 1e5\n",
    "seed = 42\n",
    "tau = 1e-2\n",
    "training_interval = 4\n",
    "lr = 5e-4  # NEW lr as parameter\n",
    "\n",
    "# NEW : pass lr to main()\n",
    "dqn = main(batch_size, gamma, buffer_size, seed, tau, training_interval, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(agent):\n",
    "    environment = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "    env = gym.wrappers.Monitor(environment, \"demo\", force=True)\n",
    "\n",
    "    done = False\n",
    "    s = environment.reset().astype(np.float32)\n",
    "    while not done:\n",
    "        env.render()\n",
    "        q_vals = agent.predict(s)\n",
    "        action = np.argmax(q_vals)\n",
    "        next_s, r, done, _ = environment.step(action)\n",
    "\n",
    "        s = next_s.astype(np.float32)\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dqn(dqn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
