{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import*\n",
    "#import matplotlib.pyplot \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from numpy import polynomial\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "#from scipy.optimize import NonlinearConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBandit:\n",
    "    \n",
    "    def __init__(self, S_space, X , Theta, sigma,seed=None):\n",
    "        #Attention! X est une liste de longueur K, chaque élément est un tableau donnant les phi(s) pour \n",
    "        #chaque s dans un intervalle des contextes S (ici discrétisé par S_space)\n",
    "        #Theta est un tableau K x d, ou chanque ligne représente les theta permettant de générer la fonction \n",
    "        #f de l'action k.\n",
    "        \n",
    "        self.S_space = np.copy(S_space)\n",
    "        \n",
    "        self.n_S = len(self.S_space)\n",
    "        \n",
    "        self.X = np.copy(X)\n",
    "        \n",
    "        self.Theta = np.copy(Theta)\n",
    "        \n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.random = np.random.RandomState(seed)\n",
    "        \n",
    "        #Nombre d'actions de l'espace discrétisé\n",
    "        #self.K = X.shape[0]\n",
    "        self.K = Theta.shape[0]\n",
    "        self.d = Theta.shape[1]\n",
    "        \n",
    "        #self.means contiendra les moyennes (f) pour chacune des deux actions\n",
    "        self.means = np.zeros((self.K, self.n_S))\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            self.means[i] = self.X[i].dot(self.Theta[i])\n",
    "        \n",
    "        # pour conserver le regret (indicateur de la performance)\n",
    "        \n",
    "        self.k_star  = np.zeros(self.n_S,dtype=int)\n",
    "        \n",
    "        self.gaps = np.zeros((self.K, self.n_S))\n",
    "        \n",
    "        for i in range(self.n_S):\n",
    "            self.k_star[i] = np.argmax(self.means[:,i])\n",
    "            self.gaps[:,i] = self.means[self.k_star[i],i] - self.means[:,i]\n",
    "        \n",
    "        #self.k_star contient l'action optimale dépendante du contexte\n",
    "        #self.gaps contient les gaps (tableau de dimension K x n_space)\n",
    "        \n",
    "        self.regret = []\n",
    "        \n",
    "    def play(self, k, s):\n",
    "        #joue l'action k au contexte s donnée, Attention, s est un index\n",
    "        self.regret.append(self.gaps[k,s])\n",
    "        \n",
    "        # Reward est la moyenne de l'action k au contexte s plus un bruit gaussien\n",
    "        reward = self.means[k,s] + self.random.normal(0, self.sigma)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def get_cumulative_regret(self):\n",
    "        \n",
    "        return np.cumsum(self.regret)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_ts_sampling_contextuel(bandit, T, Lambda, R, seed=None, plot=False):\n",
    "    '''\n",
    "    Reçoit un bandit contextue, un horizon T\n",
    "    Lambda =...\n",
    "    R = upper bound sur le sigma du bruit\n",
    "    '''\n",
    "    \n",
    "    generation = np.random.RandomState(seed)\n",
    "    \n",
    "    bandwidth = 1\n",
    "    \n",
    "    #On préparer en X et un Y pour chaque bras pour le Gaussianprocess.\n",
    "    X, y = [], []\n",
    "    \n",
    "    for k in range(bandit.K):\n",
    "        X.append([])\n",
    "        y.append([])\n",
    "    \n",
    "    #On doit initialiser un f_hat et un SIGMA_hat pour chacun des k bras\n",
    "    \n",
    "    f_hat = np.zeros((bandit.K,bandit.n_S))\n",
    "    \n",
    "    SIGMA_hat = []\n",
    "    for k in range(bandit.K):\n",
    "        SIGMA_hat.append([])\n",
    "    \n",
    "    for k in range(bandit.K):\n",
    "       # SIGMA_hat[k] = np.eye(bandit.n_S)\n",
    "        SIGMA_hat[k] = np.zeros((bandit.n_S,bandit.n_S))\n",
    "        \n",
    "    #préparation d'une liste de f_tilde près à recevoir les nombres échantillonnées pour chaque action k\n",
    "    f_tilde = np.zeros((bandit.K,bandit.n_S))\n",
    "    \n",
    "   # for (k in range(bandit.K)):\n",
    "   #     f_tilde.append([])\n",
    "    \n",
    "    for t in range(T):\n",
    "        \n",
    "        #print(t)\n",
    "        #print(X)\n",
    "        \n",
    "        #échantillonner un f_tilde pour chacune des actions.\n",
    "        \n",
    "        for k in range(bandit.K):\n",
    "            f_tilde[k] = generation.multivariate_normal(f_hat[k], SIGMA_hat[k])\n",
    "        \n",
    "        if(plot):\n",
    "            #plt.plot(bandit.S_space, f_tilde[0], color='red', label='action 1')\n",
    "            #plt.plot(bandit.S_space, f_tilde[1], color='blue', label='action 2')\n",
    "            #plt.title(\"Graphiques de la simulation de la fonction f_tilde \\n des rewards après {} tour(s)\".format(t))\n",
    "            plt.plot(bandit.S_space, f_hat[0], color='red', label='action 1')\n",
    "            plt.plot(bandit.S_space, f_hat[1], color='blue', label='action 2')\n",
    "            plt.title(\"Graphiques de la simulation de la fonction f_hat \\n des rewards après {} tour(s)\".format(t))\n",
    "            plt.xlabel('Contexte')\n",
    "            plt.ylabel('Moyenne des rewards')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        #on génère le contexte aléatoirement, (c'est un index de la discrétisation de l'espace S)\n",
    "        s = generation.randint(low=0,high=bandit.n_S)\n",
    "        \n",
    "        # l'agent infère l'action optimale selon le contexte s\n",
    "        \n",
    "        k_t = np.argmax(f_tilde[:,s])\n",
    "        #print(f_tilde[:,s])\n",
    "        \n",
    "        #l'agent jour l'action qu'il pressent optimale\n",
    "        \n",
    "        y_t = bandit.play(k_t,s)\n",
    "        \n",
    "        # mise à jour du X et du y pour l'action jouée\n",
    "        \n",
    "        X[k_t].append([bandit.S_space[s]])\n",
    "        \n",
    "        #print(t)\n",
    "        #print(k_t)\n",
    "        #print(bandit.S_space[s])\n",
    "        \n",
    "        y[k_t].append(y_t)\n",
    "        \n",
    "        # nouvelle régression pour l'action jouée\n",
    "        X_arr, y_arr = np.array(X[k_t]), np.array(y[k_t])\n",
    "            \n",
    "        model = GaussianProcessRegressor(RBF(length_scale=bandwidth), alpha=Lambda, optimizer=None)\n",
    "        \n",
    "        model.fit(X_arr, y_arr)\n",
    "        \n",
    "        # mise à jour de f_hat et SIGMA_hat pour l'action jouée\n",
    "        \n",
    "        f_hat[k_t], k_lambda = model.predict(bandit.S_space[:, None], return_cov=True)\n",
    "        \n",
    "        SIGMA_hat[k_t] = (R/Lambda)*k_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Préparation d'un contexte pour tester l'algorithme de kernel Thompson Sampling sur des bandits contextuels.\n",
    "\n",
    "space = np.linspace(0,1,100)\n",
    "\n",
    "features_1 = np.array([[1, x, x**2, x**3, x**4, x**5] for x in space])\n",
    "features_2 = np.array([[1, x, x**2, x**3, x**4, x**5] for x in space])\n",
    "\n",
    "features = [features_1,features_2]\n",
    "\n",
    "random = np.random.RandomState(999)\n",
    "theta_1 = random.uniform(-1,1,6)\n",
    "theta_2 = random.uniform(-1,1,6)\n",
    "\n",
    "\n",
    "theta_1 = theta_1/np.linalg.norm(theta_1,2)\n",
    "theta_2 = theta_2/np.linalg.norm(theta_2,2)\n",
    "\n",
    "Theta=np.array([theta_1,theta_2])\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "bandit = ContextBandit(space, features, Theta, sigma, 42)\n",
    "\n",
    "f_1 = np.dot(features[0], Theta[0])\n",
    "f_2 = np.dot(features[1], Theta[1])\n",
    "\n",
    "plt.plot(space,f_1,color='red',label='action 1')\n",
    "plt.plot(space,f_2,color='blue',label='action 2')\n",
    "plt.title('Fonctions donnant la moyenne de reward des actions\\n selon le contexte')\n",
    "plt.xlabel('Contexte')\n",
    "plt.ylabel('moyenne de reward')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#print(bandit.gaps)\n",
    "\n",
    "T=200\n",
    "Lambda=sigma**2\n",
    "R=sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test de l'algorithme\n",
    "kernel_ts_sampling_contextuel(bandit, T, Lambda, R, seed=2)\n",
    "\n",
    "plt.plot(bandit.get_cumulative_regret())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Préparation d'un contexte pour tester l'algorithme de kernel Thompson Sampling sur des bandits contextuels.\n",
    "\n",
    "space = np.linspace(0,1,100)\n",
    "\n",
    "features_1 = np.array([[1, x, x**2, x**3, x**4, x**5] for x in space])\n",
    "features_2 = np.array([[1, x, x**2, x**3, x**4, x**5] for x in space])\n",
    "\n",
    "features = [features_1,features_2]\n",
    "\n",
    "random = np.random.RandomState(999)\n",
    "theta_1 = random.uniform(-1,1,6)\n",
    "theta_2 = random.uniform(-1,1,6)\n",
    "\n",
    "\n",
    "theta_1 = theta_1/np.linalg.norm(theta_1,2)\n",
    "theta_2 = theta_2/np.linalg.norm(theta_2,2)\n",
    "\n",
    "Theta=np.array([theta_1,theta_2])\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "bandit = ContextBandit(space, features, Theta, sigma, 42)\n",
    "\n",
    "f_1 = np.dot(features[0], Theta[0])\n",
    "f_2 = np.dot(features[1], Theta[1])\n",
    "\n",
    "plt.plot(space,f_1,color='red',label='action 1')\n",
    "plt.plot(space,f_2,color='blue',label='action 2')\n",
    "plt.title('Fonctions donnant la moyenne de reward des actions\\n selon le contexte')\n",
    "plt.xlabel('Contexte')\n",
    "plt.ylabel('moyenne de reward')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#print(bandit.gaps)\n",
    "\n",
    "T=100\n",
    "Lambda=sigma**2\n",
    "R=sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test de l'algorithme avec matrice de variances, covariances nulles au départ\n",
    "kernel_ts_sampling_contextuel(bandit, T, Lambda, R, seed=2, plot=True)\n",
    "\n",
    "plt.plot(bandit.get_cumulative_regret())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
