{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les librairies nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "from poutyne import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'assure de fixer tous les générateurs de nombres aléatoires pour avoir une reproductibilité de nos expériences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(environment, seed):\n",
    "    environment.seed(seed)\n",
    "    environment.action_space.seed(seed)\n",
    "    environment.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "environment = gym.make(\"CartPole-v1\")\n",
    "set_random_seed(environment, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On implémente notre fonction de représentation des paires (état, action) qui joue sur la symmétrie de l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01258566,  0.00156614, -0.04207708,  0.00180545])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phi(state, action):\n",
    "    \"\"\"\n",
    "    Returns state if the action is 1, -state if not.\n",
    "    \"\"\"\n",
    "    return state if action == 1 else -state\n",
    "\n",
    "\n",
    "# Example call\n",
    "state = environment.reset()\n",
    "action = 0\n",
    "phi(state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise notre fonction phi précédente pour calculer les Q-values d'un état s selon une liste d'actions et un approximateur linéaire thêta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04139742, -0.04139742])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_fa(theta, state, actions):\n",
    "    \"\"\"\n",
    "    Takes as input a linear FA theta and a state vector.\n",
    "\n",
    "    theta and state both are (4,) numpy arrays.\n",
    "    \n",
    "    Actions is a list of integers representing the available discrete actions (e.g. [0, 1, 2, 3]).\n",
    "\n",
    "    Returns the estimated q-values of each (s,a) pair according to the dot-product <theta, phi(s,a)>.\n",
    "\n",
    "    The return is a (2,) numpy array.\n",
    "    \"\"\"\n",
    "    # On commence par chercher tous les phis\n",
    "    phis = np.array([phi(state, action) for action in actions])\n",
    "\n",
    "    # On calcule le produit vectoriel <theta, phi>\n",
    "    action_vals = phis.dot(theta)\n",
    "\n",
    "    return action_vals\n",
    "\n",
    "\n",
    "# Example call\n",
    "theta = np.random.normal(size=environment.observation_space.shape)\n",
    "state = environment.reset()\n",
    "actions = list(range(environment.action_space.n))\n",
    "linear_fa(theta, state, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour effectuer la mise à jour des poids thêta, on doit calculer les cibles pour notre réseau selon la formule du one-step SARSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01016668, -0.34985956, -1.18778339, -1.02107105])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_targets(next_q_vals, rewards, terminal, gamma):\n",
    "    \"\"\"\n",
    "    Returns Q-learning targets according to the 1-step SARSA lookahead formula.\n",
    "    i.e. target_t = r_t + gamma * max(Q(s_t+1))\n",
    "\n",
    "    If s_t was already terminal, then we only have target_t = r_t.\n",
    "\n",
    "    next_q_vals is a (batch_size, 2) numpy array representing the Q(s_t+1) values\n",
    "    rewards is a (batch_size,) numpy array representing the r_t values\n",
    "    terminal is a (batch_size,) boolean numpy array representing if s_t+1 was terminal\n",
    "    gamma is a float between 0 and 1\n",
    "\n",
    "    Returns a (batch_size,) numpy array containing the one-step lookahead targets.\n",
    "\n",
    "    N.B. Most of the code here can be reused in your TP2.\n",
    "    \"\"\"\n",
    "    next_actions_vals_selected = np.max(next_q_vals, axis=1)\n",
    "    \n",
    "    targets = rewards + gamma * next_actions_vals_selected * (1 - terminal)\n",
    "    \n",
    "    return targets\n",
    "\n",
    "\n",
    "# Example call\n",
    "batch_size = 4\n",
    "next_q_vals = np.random.normal(size=(batch_size,2))\n",
    "rewards = np.random.normal(size=(batch_size,))\n",
    "terminal = np.random.randint(low=0, high=1, size=(batch_size,)).astype(bool)\n",
    "gamma = 0.99\n",
    "get_targets(next_q_vals, rewards, terminal, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez maintenant la règle de mise à jour des poids thêta que vous avez trouvée à la question 1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta before update : [-1.01283112  0.31424733 -0.90802408 -1.4123037 ]\n",
      "Theta after update : [-0.97083916  0.28462365 -0.88133645 -1.41826024]\n"
     ]
    }
   ],
   "source": [
    "def update_theta(theta, states, actions_taken, targets, q_vals, lr):\n",
    "    \"\"\"\n",
    "    Updates theta according to the gradient descent on the MSE error between target and \n",
    "    predicted q_vals. The update is made according to the analytic update you found in question 1.\n",
    "\n",
    "    theta is a (4,) numpy array representing the linear FA.\n",
    "    states is a (batch_size, 4) numpy array representing the s_t used in the batch\n",
    "    actions_taken is a (batch_size,) numpy array representing the a_t actions in the batch\n",
    "    targets is a (batch_size,) numpy array representing the one-step lookahead targets target_t\n",
    "    q_vals is a (batch_size, 1) numpy array representing the q(s_t,a) values for each time step\n",
    "    lr is the learning rate hyperparameter.\n",
    "\n",
    "    Updates theta in-place.\n",
    "    \"\"\"\n",
    "    predicted = np.take(q_vals, actions_taken)\n",
    "    diff = targets - predicted\n",
    "    \n",
    "    diff = diff[:, np.newaxis]\n",
    "    \n",
    "    phis = np.array([phi(state, action) for state, action in zip(states, actions_taken)])\n",
    "    \n",
    "    grad = -2 * (diff * phis).sum(0)\n",
    "    grad /= len(states)\n",
    "               \n",
    "    theta -= lr * grad\n",
    "\n",
    "\n",
    "# Example call for batch_size = 4\n",
    "batch_size = 4\n",
    "n_actions = environment.action_space.n\n",
    "theta = np.random.normal(size=environment.observation_space.shape)\n",
    "states = np.array([environment.reset() for _ in range(batch_size)])\n",
    "actions_taken = np.random.randint(low=0, high=n_actions, size=(batch_size,))\n",
    "targets = np.random.normal(size=(batch_size,))\n",
    "q_vals = np.random.normal(size=(batch_size, n_actions))\n",
    "lr = 1.0\n",
    "\n",
    "print(f\"Theta before update : {theta}\")\n",
    "update_theta(theta, states, actions_taken, targets, q_vals, lr)\n",
    "print(f\"Theta after update : {theta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va générer nos trajectoires avec une politique epsilon-greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def epsilon_greedy_policy(q_vals, action_space, epsilon):\n",
    "    \"\"\"\n",
    "    Selects an action according to an epsilon-greedy policy.\n",
    "\n",
    "    q_vals is a (2,) numpy array where q_val[i] represents the estimated q_value for action i.\n",
    "    \n",
    "    action_space is the ActionSpace object from an OpenAI gym environment.\n",
    "    One can randomly from an action_space via action_space.sample().\n",
    "\n",
    "    epsilon (between 0 and 1) represents the odd of selecting the action randomly.\n",
    "    If the action is not selected randomly, it is selected as the argmax of the q_vals.\n",
    "\n",
    "    Returns an integer action.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(q_vals)\n",
    "\n",
    "\n",
    "# Example call\n",
    "q_vals = np.random.normal(size=(2,))\n",
    "action_space = environment.action_space\n",
    "epsilon = 0.5\n",
    "epsilon_greedy_policy(q_vals, action_space, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, comme on est off-policy quand on fait du Q-learning, on fait appel à un replay buffer qui entrepose un nombre fixe de transitions vues et à partir desquelles on va s'entraîner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay buffer object that stores elements up until a certain maximum size.\n",
    "\n",
    "    N.B. Most of the code here can be reused in your TP2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\"\n",
    "        Init the buffer and store buffer_size property.\n",
    "        \"\"\"\n",
    "        self.__buffer_size__ = buffer_size\n",
    "        self.data = []\n",
    "\n",
    "    def store(self, element):\n",
    "        \"\"\"\n",
    "        Stores an element.\n",
    "\n",
    "        If the buffer is already full, pop the oldest element inside.\n",
    "        \"\"\"\n",
    "        self.data.append(element)\n",
    "        \n",
    "        if len(self.data) > self.__buffer_size__:\n",
    "            del self.data[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly samples batch_size elements from the buffer.\n",
    "\n",
    "        Returns the list of sampled elements.\n",
    "        \"\"\"\n",
    "        return random.sample(self.data, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a maintenant toutes les pièces pour apprendre une FA linéaire pour l'environnement CartPole-v1.\n",
    "\n",
    "Dans votre implémentation, initialisez thêta selon une loi normale N(0,1).\n",
    "Mettez à niveau epsilon en le mutipliant par epsilon_decay après chaque trajectoire.\n",
    "Utilisez les valeurs par défaut fournies pour les autres paramètres.\n",
    "\n",
    "Lors de la génération de la trajectoire, entreposez un tuple représentant l'état courant, l'action choisie,la récompense, le prochain état et si la trajectoire est terminée après cette action dans votre replay buffer.\n",
    "\n",
    "Lorsque votre replay buffer contient au moins batch_size éléments, faites une mise à jour des poids theta à chaque pas de temps dans l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 trajectoires, we have G_0 = 38.00, 0.817073\n",
      "After 20 trajectoires, we have G_0 = 20.00, 0.667608\n",
      "After 30 trajectoires, we have G_0 = 35.00, 0.545484\n",
      "After 40 trajectoires, we have G_0 = 187.00, 0.445700\n",
      "After 50 trajectoires, we have G_0 = 102.00, 0.364170\n",
      "After 60 trajectoires, we have G_0 = 79.00, 0.297553\n",
      "After 70 trajectoires, we have G_0 = 171.00, 0.243123\n",
      "After 80 trajectoires, we have G_0 = 105.00, 0.198649\n",
      "After 90 trajectoires, we have G_0 = 122.00, 0.162311\n",
      "After 100 trajectoires, we have G_0 = 140.00, 0.132620\n",
      "After 110 trajectoires, we have G_0 = 159.00, 0.108360\n",
      "After 120 trajectoires, we have G_0 = 194.00, 0.088538\n",
      "After 130 trajectoires, we have G_0 = 129.00, 0.072342\n",
      "After 140 trajectoires, we have G_0 = 140.00, 0.059109\n",
      "After 150 trajectoires, we have G_0 = 147.00, 0.048296\n",
      "After 160 trajectoires, we have G_0 = 154.00, 0.039461\n",
      "After 170 trajectoires, we have G_0 = 148.00, 0.032243\n",
      "After 180 trajectoires, we have G_0 = 200.00, 0.026345\n",
      "After 190 trajectoires, we have G_0 = 162.00, 0.021526\n",
      "After 200 trajectoires, we have G_0 = 127.00, 0.017588\n",
      "After 210 trajectoires, we have G_0 = 198.00, 0.014371\n",
      "After 220 trajectoires, we have G_0 = 157.00, 0.011742\n",
      "After 230 trajectoires, we have G_0 = 134.00, 0.010000\n",
      "After 240 trajectoires, we have G_0 = 143.00, 0.010000\n",
      "After 250 trajectoires, we have G_0 = 134.00, 0.010000\n"
     ]
    }
   ],
   "source": [
    "def batch_linear_q(\n",
    "    gamma=0.99,\n",
    "    n_trajectories=300,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    buffer_size=50000,\n",
    "    seed=42,\n",
    "    epsilon_decay=0.98,\n",
    "    epsilon_min=0.01,\n",
    "):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    set_random_seed(environment, seed)\n",
    "\n",
    "    # Theta init selon une loi normale\n",
    "    theta = np.random.normal(size=environment.observation_space.shape)\n",
    "    actions = list(range(environment.action_space.n))\n",
    "\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    epsilon = 1.0\n",
    "    for n_trajectories in range(n_trajectories):        \n",
    "        trajectory_done = False\n",
    "        \n",
    "        s= environment.reset()\n",
    "        G = 0\n",
    "        \n",
    "        while not trajectory_done:\n",
    "            q_vals = linear_fa(theta, s, actions)\n",
    "            \n",
    "            a = epsilon_greedy_policy(q_vals, environment.action_space, epsilon)\n",
    "            next_s, r, trajectory_done, _ = environment.step(a)\n",
    "            \n",
    "            G += r\n",
    "            \n",
    "            replay_buffer.store((s, a, r, next_s, trajectory_done))\n",
    "            \n",
    "            s = next_s\n",
    "            \n",
    "            if len(replay_buffer.data) > batch_size:\n",
    "                minibatch = replay_buffer.get_batch(batch_size)\n",
    "                \n",
    "                states = np.array([x[0] for x in minibatch])\n",
    "                actions_taken = np.array([x[1] for x in minibatch])\n",
    "                rewards = np.array([x[2] for x in minibatch])\n",
    "                next_states = np.array([x[3] for x in minibatch])\n",
    "                dones = np.array([x[4] for x in minibatch])\n",
    "                \n",
    "                next_q_vals_predicted = np.array([linear_fa(theta, s, actions) for s in next_states])\n",
    "                targets = get_targets(next_q_vals_predicted, rewards, dones, gamma)\n",
    "                \n",
    "                q_vals_predicted = np.array([linear_fa(theta, s, actions) for s in states])\n",
    "                \n",
    "                update_theta(theta, states, actions_taken, targets, q_vals_predicted, lr)\n",
    "                \n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        if (n_trajectories+1) % 10 == 0:\n",
    "            print(f\"After {n_trajectories+1} trajectoires, we have G_0 = {G:.2f}, {epsilon:4f}\")\n",
    "            \n",
    "    return theta\n",
    "\n",
    "batch_linear_q(n_trajectories = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fa(theta):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    set_random_seed(environment, seed=42)\n",
    "    \n",
    "    env = gym.wrappers.Monitor(environment, \"demo\", force=True)\n",
    "    \n",
    "    done = False\n",
    "    s = environment.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        q_vals = linear_fa(theta, s, actions)\n",
    "        action = np.argmax(q_vals)\n",
    "        next_s, r, done, _ = environment.step(action)\n",
    "        \n",
    "        s = next_s\n",
    "    env.close()\n",
    "run_fa(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fa(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant comment on peut apprendre une approximation pour la Q-function avec des réseaux de neurones !\n",
    "\n",
    "On commmence par se créer une réseau de neurones. Pour les besoins du cours,\n",
    "on vous fournit une implémentation simple qui utilise la librairie de réseaux de neurones [Pytorch](https://pytorch.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NNModel(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_hidden_layers=1, hidden_dim=16):\n",
    "        \"\"\"\n",
    "        Builds a PyTorch Neural Network with n_hidden_layers, all of hidden_dim neurons.\n",
    "\n",
    "        The activation function is always ReLU for intermediate layers and the final \n",
    "        layer does not have any activation function.\n",
    "\n",
    "        By default, this NN only has one hidden layer of 16 neurons.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers = [torch.nn.Linear(in_dim, hidden_dim), torch.nn.ReLU()]\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.extend([torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU()])\n",
    "        layers.append(torch.nn.Linear(hidden_dim, out_dim))\n",
    "\n",
    "        self.fa = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the function that is called when we want to get the output f(x)\n",
    "        for our NN f.\n",
    "        \"\"\"\n",
    "        return self.fa(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez ici que l'on change quelque peu la formulation de notre approximation de fonction.\n",
    "\n",
    "Notre réseau de neurones prend maintenant en entrée un état (sans représentation de fonction) et retourne un vecteur représentant la Q-value estimée pour chaque action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On utilisera aussi la libraire [Poutyne](https://poutyne.org/), qui est essentiellement un wrapper autour de PyTorch, permettant de s'éviter d'avoir à réécrire toujours la même poutine de code PyTorch. \n",
    "\n",
    "Voici quelques exemples des commandes à connaître par rapport à Poutyne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Au coeur de Poutyne se trouve le principe de modèle, qui correspond à un réseau de \n",
    "# neurones, accompagné de son optimiseur et de sa fonction de perte.\n",
    "\n",
    "# On crée notre réseau de neurones\n",
    "network = NNModel(environment.observation_space.shape[0], environment.action_space.n)\n",
    "\n",
    "# Notre optimiseur: on choisit Adam par défaut, que l'on initialise pour optimiser\n",
    "# les poids de notre réseau avec un learning rate de 1e-1.\n",
    "optim = torch.optim.Adam(network.parameters(), lr=1e-1)\n",
    "\n",
    "# Pour notre fonction de perte, Poutyne supporte de prendre en entrée des string \n",
    "# représentant des pertes habituelles, comme par \"mse\" pour la Mean Squared Error.\n",
    "model_mse = Model(network, optim, loss_function=\"mse\")\n",
    "\n",
    "# On va chercher une première observation pour tester notre modèle.\n",
    "x = environment.reset()\n",
    "\n",
    "# On doit mettre l'observation en float32 car les paramètres d'un réseau PyTorch sont de ce type\n",
    "# par défaut.\n",
    "x = x.astype(np.float32)\n",
    "\n",
    "# On peut obtenir les valeurs prédites par notre réseau sur notre état initial en appelant predict()\n",
    "print(f\"Q-valeurs pour l'état initial : {model_mse.predict(x)}\")\n",
    "\n",
    "# Pour l'entraînement, il faut maintenant passer des entrées x et des cibles y.\n",
    "# Pour les besoins de l'exemple, prenons comme cible un vecteur de 1.\n",
    "y = np.ones((2,))\n",
    "\n",
    "# Il est important de mettre aussi la cible en float32 pour les besoins de PyTorch\n",
    "y = y.astype(np.float32)\n",
    "print(f\"On utilisera la cible {y}\")\n",
    "\n",
    "# On appelle la fonction train_on_batch qui fait la mise à jour des poids du réseau\n",
    "# selon la batch (x, y) passée en paramètre.\n",
    "# La fonction train_on_batch() retourne la perte observée sur le batch passée en paramètre\n",
    "loss = model_mse.train_on_batch(x, y)\n",
    "print(f\"La perte pour la première batch est {loss}\")\n",
    "\n",
    "# On peut voir que le réseau a été mis à jour en redemandant à notre réseau de prédire x\n",
    "# Le réseau se rapproche ainsi de la cible y\n",
    "print(f\"Q-valeurs pour l'état initial après la mise à jour : {model_mse.predict(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a maintenant toutes les pièces pour apprendre une FA neuronale pour l'environnement CartPole-v1.\n",
    "\n",
    "Dans votre implémentation, mettez à niveau epsilon en le mutipliant par epsilon_decay après chaque trajectoire.\n",
    "Utilisez les valeurs par défaut fournies pour les autres paramètres.\n",
    "\n",
    "Lors de la génération de la trajectoire, entreposez un tuple représentant l'état courant, l'action choisie,la récompense, le prochain état et si la trajectoire est terminée après cette action dans votre replay buffer.\n",
    "\n",
    "Lorsque votre replay buffer contient au moins batch_size éléments, faites une mise à jour des poids theta à chaque pas de temps dans l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dqn_loss(y_pred, y_target):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - y_pred, (batch_size, n_actions) Tensor outputted by the network\n",
    "        - y_target = (actions, Q_target), where actions and targets both\n",
    "                      are Tensors with the shape (batch_size, ). \n",
    "                      Actions are the selected actions according to the target network\n",
    "                      and targets are the one-step lookahead targets.\n",
    "\n",
    "    Returns :\n",
    "        - The DQN loss (same as for the linear case).\n",
    "\n",
    "    N.B. Most of the code here can be reused in your TP2.\n",
    "    \"\"\"\n",
    "    # C'est essentiellement le même travail que ce qui est fait dans update_theta\n",
    "    # sauf (1) qu'on le fait en PyTorch et les fonctions n'ont pas le même nom\n",
    "    # et (2) on ne calcule pas le gradient nous-mêmes, on fait juste donner la perte.\n",
    "    # C'est PyTorch qui fait la descente de gradient pour nous.\n",
    "    actions, q_target = y_target\n",
    "    \n",
    "    print(actions)\n",
    "    print(q_target)\n",
    "    \n",
    "    q_predicted = y_pred.gather(1, actions.unsqueeze(-1)).squeeze()\n",
    "    \n",
    "    return torch.nn.functional.mse_loss(q_predicted, q_target)\n",
    "\n",
    "\n",
    "def deep_q_learning(\n",
    "    gamma=0.99,\n",
    "    n_trajectories=300,\n",
    "    batch_size=128,\n",
    "    lr=1e-3,\n",
    "    buffer_size=50000,\n",
    "    seed=42,\n",
    "    epsilon_decay=0.98,\n",
    "    epsilon_min=0.01,\n",
    "):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    set_random_seed(environment, seed)\n",
    "\n",
    "    # On crée le réseau de neurones\n",
    "    model = NNModel(environment.observation_space.shape[0], environment.action_space.n)\n",
    "\n",
    "    # On utilise la librairie Poutyne en lui passant notre réseau.\n",
    "    # Notez que l'on précise aussi un optimiseur (Adam) pour les paramètres de notre\n",
    "    # réseau, pour lequel on définit le learning_rate à 1e-3 ici.\n",
    "    # On fournit aussi la fonction de perte dqn_loss au modèle Poutyne pour lui dire\n",
    "    # comment son apprentissage doit se faire.\n",
    "    agent = Model(\n",
    "        model, torch.optim.Adam(model.parameters(), lr=lr), loss_function=dqn_loss\n",
    "    )\n",
    "\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    epsilon = 1.0\n",
    "\n",
    "    for n_trajectories in range(n_trajectories):\n",
    "        trajectory_done = False\n",
    "        \n",
    "        s = environment.reset().astype(np.float32)\n",
    "        G = 0\n",
    "        \n",
    "        while not trajectory_done:\n",
    "            q_vals = agent.predict(s)\n",
    "            \n",
    "            a = epsilon_greedy_policy(q_vals, environment.action_space, epsilon)\n",
    "            next_s, r, trajectory_done, _ = environment.step(a)\n",
    "            next_s = next_s.astype(np.float32)\n",
    "            \n",
    "            G += r\n",
    "            replay_buffer.store((s, a, r, next_s, trajectory_done))\n",
    "            s = next_s\n",
    "            \n",
    "            if len(replay_buffer.data) > batch_size:\n",
    "                minibatch = replay_buffer.get_batch(batch_size)\n",
    "                \n",
    "                states = np.array([x[0] for x in minibatch])\n",
    "                actions_taken = np.array([x[1] for x in minibatch])\n",
    "                rewards = np.array([x[2] for x in minibatch])\n",
    "                next_states = np.array([x[3] for x in minibatch])\n",
    "                dones = np.array([x[4] for x in minibatch])\n",
    "                \n",
    "                next_q_vals_predicted = agent.predict(next_states)\n",
    "                targets = get_targets(next_q_vals_predicted, rewards, dones, gamma).astype(np.float32)\n",
    "                \n",
    "                agent.train_on_batch(states, (actions_taken, targets))\n",
    "        \n",
    "        \n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        if (n_trajectories+1) % 10 == 0:\n",
    "            print(f\"After {n_trajectories+1} trajectoires, we have G_0 = {G:.2f}, {epsilon:4f}\")\n",
    "    return agent\n",
    "\n",
    "agent = deep_q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(agent):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    set_random_seed(environment, seed=42)\n",
    "    \n",
    "    env = gym.wrappers.Monitor(environment, \"demo\", force=True)\n",
    "    \n",
    "    done = False\n",
    "    s = environment.reset().astype(np.float32)\n",
    "    while not done:\n",
    "        env.render()\n",
    "        q_vals = agent.predict(s)\n",
    "        action = np.argmax(q_vals)\n",
    "        next_s, r, done, _ = environment.step(action)\n",
    "        \n",
    "        s = next_s.astype(np.float32)\n",
    "    env.close()\n",
    "run_dqn(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
