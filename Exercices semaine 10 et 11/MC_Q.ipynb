{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les librairies qui seront nécessaires. gym est la librairie de OpenAI qui contient les environnements de benchmark pour les algos de RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from math import radians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer un environnement, on fait appel à gym.make(\"NOM_ENVIRONNEMENT\").\n",
    "\n",
    "Voici quelques exemples de commandes avec l'environnement CartPole-v1, où les états sont continus et les actions discrètes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'environnement CartPole-v1 a des observations de la forme (4,) et contient 2 actions.\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# On extrait l'état de départ. Pour CartPole, le départ est stochastique: \n",
    "# le poteau et le chariot ne sont pas toujours aux mêmes points de départ.\n",
    "s0 = environment.reset()\n",
    "\n",
    "# L'environnement contient un attribut observation_space qui qualifie\n",
    "# l'espace d'états. \n",
    "# Pour les états continus comme ici, on a accès à la proprité shape pour \n",
    "# connaître la forme des observations.\n",
    "observation_dim = environment.observation_space.shape\n",
    "\n",
    "# L'environnement contient aussi un attribut action_space qui qualifie\n",
    "# l'espace d'actions.\n",
    "# Pour les espaces à actions discrètes comme CartPole, celui-ci contient un \n",
    "# attribut n indiquant le nombre d'actions et les actions sont tout\n",
    "# simplement les entiers de [n].\n",
    "n_actions = environment.action_space.n # Only discrete action spaces\n",
    "\n",
    "# On fait un pas dans l'environnement avec la fonction step(action).\n",
    "# Cette fonction retourne (s, r, done), qui représentent respectivement\n",
    "# le prochain état, la récompense et le fait que la trajectoire est finie ou pas.\n",
    "s, r, done, _ = environment.step(1) # Actions are int\n",
    "\n",
    "# Infos de base sur l'environnement\n",
    "print(f\"L'environnement {environment.spec.id} a des observations de la forme {observation_dim} et contient {n_actions} actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la reproductibilité, on veut être capables d'initiliaser notre germe de nombres aléatoire à une valeur fixée.\n",
    "\n",
    "Voici une fonction qui le fait pour vous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(environment, seed):\n",
    "    environment.seed(seed)\n",
    "    environment.action_space.seed(seed)\n",
    "    environment.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_random_seed(environment, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour gérer de manière tabulaire un espace d'états continu comme celui de CartPole, on procède à une discrétisation de l'espace.\n",
    "\n",
    "La première étape d'une discrétisation est de déterminer les limites de l'espace d'état.\n",
    "\n",
    "Celles-ci sont accessibles par \"environment.observation_space.high\" et \"environment.observation_space.low\".\n",
    "\n",
    "Dans notre cas, l'état représente \\[position chariot, vitesse chariot, angle poteau, vitesse angulaire poteau\\].\n",
    "\n",
    "Comme les vitesses ont des bornes infinies qui ne prètent pas bien à la discrétisation, on leur fixe les valeurs limites de 0.5 unité / t et 50 radians / t pour la vitesse du chariot et du poteau, respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bounds = environment.observation_space.high\n",
    "upper_bounds[1] = 0.5\n",
    "upper_bounds[3] = radians(50)\n",
    "\n",
    "lower_bounds = -upper_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la discrétisation, nous vous suggérons maintenant de discrétiser chaque composante en \\[1, 1, 6, 12\\] zones respectivement.\n",
    "\n",
    "Votre discrétisation devrait retourner un tuple de taille 4 où chaque indice correspond à l'indice de la zone discrète représentant la composante continue. \n",
    "\n",
    "Chaque zone est de taille identique : (upper_i - lower_i) / N_i, où upper_i et lower_i sont les bornes supérieure et inférieure pour la composante i et N_i est le nombre de zones qui séparent la composante i.\n",
    "\n",
    "Assurez-vous de gérer les cas limites où l'état en entrée contient des composantes en dehors de vos bornes (ramenez les à la borne la plus près)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(state, n_buckets=[1, 1, 6, 12]):\n",
    "    \"\"\"\n",
    "    Takes as input a (4,) state of the CartPole environment and returns a (4,) tuple\n",
    "    of discrete indexes for each component.\n",
    "\n",
    "    The number of components is set by the n_buckets parameter and the split is \n",
    "    made according to the lower_bounds and upper_bounds global values.\n",
    "    \"\"\"\n",
    "    \n",
    "    #S'assurer que tout est dans les bornes.\n",
    "    \n",
    "    state = np.maximum(state, lower_bounds)\n",
    "    state = np.minimum(state, upper_bounds)\n",
    "    \n",
    "    \n",
    "    #Normalisation entre 0, 1 l'état à chaque index\n",
    "    x = (state-lower_bounds)/(upper_bounds-lower_bounds)\n",
    "    \n",
    "    #transformation en un nombre entier entre 0 et n-1.\n",
    "    \n",
    "    x = x*(np.array(n_buckets)-1)\n",
    "    \n",
    "    discrete_x = np.rint(x).astype(int) # le astype pour qu'il puisse être vue comme un entier (un index) plus tard\n",
    "    \n",
    "    return tuple(discrete_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On bâtit maintenant un tableau qui va contenir nos approximations des Q valeurs selon la mise à jour Monte-Carlo.\n",
    "\n",
    "Ce tableau sera de taille \\[1, 1, 6, 16, 2\\] pour correspondre à la taille de notre discrétisation et au nombre d'actions disponibles.\n",
    "\n",
    "Implémentez la mise à jour des valeurs selon la formule de la moyenne mobile décrit à la section 2.4 du livre de Sutton et Barto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, discrete_obs_shape, n_actions):\n",
    "        \"\"\"\n",
    "        Initializes two [discrete_obs_shape, n_actions] ndarrays of zeros.\n",
    "        One is to store the current q estimated values and the other is to store \n",
    "        the number of visits\n",
    "        \"\"\"\n",
    "        #dimension du teableau est un tuple former de la conténation de discrete_obs_shape et n_actions\n",
    "        #Typiquement, ici la shape est (1,1,6,16,2).\n",
    "        \n",
    "        self.table_shape= discrete_obs_shape + (n_actions,)\n",
    "        \n",
    "        #Tableaux de 0 pour initialiser les valeurs de Q\n",
    "        self.Q=np.zeros(self.table_shape, dtype=np.float32)\n",
    "        self.N_visits=np.zeros(self.table_shape,dtype=np.uint)\n",
    "        \n",
    "    def store_return(self, obs_index, action, g):\n",
    "        \"\"\"\n",
    "        Stores return g for (obs_index, action) pair.\n",
    "        Updates the corresponding q value according to recurrent running\n",
    "        mean formula.\n",
    "        Also updates number of visits to said pair.\n",
    "        \"\"\"\n",
    "        #g est le gain cumulé\n",
    "        \n",
    "        #Obention de l'index de la table Q à utiliser.\n",
    "        Q_idx=obs_index + (action,)\n",
    "        \n",
    "        #Q value précédente associée à la paire (état-action)\n",
    "        prev_Q_value=self.Q[Q_idx]\n",
    "        \n",
    "        #Nombre de visites qu'on avait pour cet élément là.\n",
    "        prev_N_visites = self.N_visits[Q_idx]\n",
    "        \n",
    "        #Mise à jour de la valeur de Q pour la paire (état-action) \n",
    "        #à l'aide la formule donnée par la section 2.4 du livre de Sutton et Barto.\n",
    "        \n",
    "        #self.Q[Q_idx]=prev_Q_value+(1/(prev_N_visites+1))*(g-prev_Q_value)\n",
    "        \n",
    "        #Méthode d'Audrey\n",
    "        self.Q[Q_idx]=(prev_Q_value*prev_N_visites+g)/(prev_N_visites+1)\n",
    "        \n",
    "        #Mise à journ du nombre de visite pour la paire état-action\n",
    "        self.N_visits[Q_idx] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On implémente aussi une politique epsilon-greedy. \n",
    "\n",
    "Notez qu'on peut piger une action aléatoire de l'environnement avec \"action_space.sample()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state_idx, table, epsilon, action_space):\n",
    "    \"\"\"\n",
    "    Takes as input a state_idx, a Table table, an epsilon and an action_space parameters.\n",
    "\n",
    "    table has a q attribute that can be read with table.q[state_idx] to get the estimated\n",
    "    q values of all actions at this specific state index.\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(table.Q[state_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez maintenant l'algorithme MC Control First-visit de la section 5.4 du livre de Sutton.\n",
    "\n",
    "Utilisez les fonctions discretize(), choose_action() et l'objet Table que vous venez de créer.\n",
    "\n",
    "Faites changer epsilon en le faisant commencer à 1.0 et en le multipliant par 0.99 à la fin de chaque épisode. Conservez un epsilon minimal de 0.1.\n",
    "\n",
    "Générez 5000 trajectoires avec ces paramètres. Votre agent devrait converger à une récompense près de 500.\n",
    "\n",
    "La convergence est très sensible et c'est donc normal de voir votre agent obtenir des trajectoires de 500 points de récompenses suivies de trajectoires de 20 points de récompense.\n",
    "\n",
    "Retournez la table finale après votre entraînement, elle sera utile plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après 0 trajectoires, nous avons G_0 = 20.0\n",
      "Après 50 trajectoires, nous avons G_0 = 78.0\n",
      "Après 100 trajectoires, nous avons G_0 = 77.0\n",
      "Après 150 trajectoires, nous avons G_0 = 83.0\n",
      "Après 200 trajectoires, nous avons G_0 = 93.0\n",
      "Après 250 trajectoires, nous avons G_0 = 192.0\n",
      "Après 300 trajectoires, nous avons G_0 = 34.0\n",
      "Après 350 trajectoires, nous avons G_0 = 112.0\n",
      "Après 400 trajectoires, nous avons G_0 = 119.0\n",
      "Après 450 trajectoires, nous avons G_0 = 16.0\n",
      "Après 500 trajectoires, nous avons G_0 = 154.0\n",
      "Après 550 trajectoires, nous avons G_0 = 28.0\n",
      "Après 600 trajectoires, nous avons G_0 = 96.0\n",
      "Après 650 trajectoires, nous avons G_0 = 46.0\n",
      "Après 700 trajectoires, nous avons G_0 = 255.0\n",
      "Après 750 trajectoires, nous avons G_0 = 48.0\n",
      "Après 800 trajectoires, nous avons G_0 = 98.0\n",
      "Après 850 trajectoires, nous avons G_0 = 43.0\n",
      "Après 900 trajectoires, nous avons G_0 = 37.0\n",
      "Après 950 trajectoires, nous avons G_0 = 111.0\n",
      "Après 1000 trajectoires, nous avons G_0 = 28.0\n",
      "Après 1050 trajectoires, nous avons G_0 = 15.0\n",
      "Après 1100 trajectoires, nous avons G_0 = 104.0\n",
      "Après 1150 trajectoires, nous avons G_0 = 67.0\n",
      "Après 1200 trajectoires, nous avons G_0 = 62.0\n",
      "Après 1250 trajectoires, nous avons G_0 = 42.0\n",
      "Après 1300 trajectoires, nous avons G_0 = 26.0\n",
      "Après 1350 trajectoires, nous avons G_0 = 153.0\n",
      "Après 1400 trajectoires, nous avons G_0 = 42.0\n",
      "Après 1450 trajectoires, nous avons G_0 = 36.0\n",
      "Après 1500 trajectoires, nous avons G_0 = 145.0\n",
      "Après 1550 trajectoires, nous avons G_0 = 36.0\n",
      "Après 1600 trajectoires, nous avons G_0 = 161.0\n",
      "Après 1650 trajectoires, nous avons G_0 = 76.0\n",
      "Après 1700 trajectoires, nous avons G_0 = 30.0\n",
      "Après 1750 trajectoires, nous avons G_0 = 179.0\n",
      "Après 1800 trajectoires, nous avons G_0 = 21.0\n",
      "Après 1850 trajectoires, nous avons G_0 = 11.0\n",
      "Après 1900 trajectoires, nous avons G_0 = 500.0\n",
      "Après 1950 trajectoires, nous avons G_0 = 47.0\n",
      "Après 2000 trajectoires, nous avons G_0 = 18.0\n",
      "Après 2050 trajectoires, nous avons G_0 = 305.0\n",
      "Après 2100 trajectoires, nous avons G_0 = 88.0\n",
      "Après 2150 trajectoires, nous avons G_0 = 235.0\n",
      "Après 2200 trajectoires, nous avons G_0 = 44.0\n",
      "Après 2250 trajectoires, nous avons G_0 = 256.0\n",
      "Après 2300 trajectoires, nous avons G_0 = 93.0\n",
      "Après 2350 trajectoires, nous avons G_0 = 101.0\n",
      "Après 2400 trajectoires, nous avons G_0 = 196.0\n",
      "Après 2450 trajectoires, nous avons G_0 = 18.0\n",
      "Après 2500 trajectoires, nous avons G_0 = 39.0\n",
      "Après 2550 trajectoires, nous avons G_0 = 126.0\n",
      "Après 2600 trajectoires, nous avons G_0 = 176.0\n",
      "Après 2650 trajectoires, nous avons G_0 = 32.0\n",
      "Après 2700 trajectoires, nous avons G_0 = 93.0\n",
      "Après 2750 trajectoires, nous avons G_0 = 113.0\n",
      "Après 2800 trajectoires, nous avons G_0 = 104.0\n",
      "Après 2850 trajectoires, nous avons G_0 = 27.0\n",
      "Après 2900 trajectoires, nous avons G_0 = 19.0\n",
      "Après 2950 trajectoires, nous avons G_0 = 179.0\n",
      "Après 3000 trajectoires, nous avons G_0 = 64.0\n",
      "Après 3050 trajectoires, nous avons G_0 = 22.0\n",
      "Après 3100 trajectoires, nous avons G_0 = 57.0\n",
      "Après 3150 trajectoires, nous avons G_0 = 176.0\n",
      "Après 3200 trajectoires, nous avons G_0 = 11.0\n",
      "Après 3250 trajectoires, nous avons G_0 = 117.0\n",
      "Après 3300 trajectoires, nous avons G_0 = 349.0\n",
      "Après 3350 trajectoires, nous avons G_0 = 167.0\n",
      "Après 3400 trajectoires, nous avons G_0 = 46.0\n",
      "Après 3450 trajectoires, nous avons G_0 = 27.0\n",
      "Après 3500 trajectoires, nous avons G_0 = 159.0\n",
      "Après 3550 trajectoires, nous avons G_0 = 206.0\n",
      "Après 3600 trajectoires, nous avons G_0 = 115.0\n",
      "Après 3650 trajectoires, nous avons G_0 = 130.0\n",
      "Après 3700 trajectoires, nous avons G_0 = 128.0\n",
      "Après 3750 trajectoires, nous avons G_0 = 133.0\n",
      "Après 3800 trajectoires, nous avons G_0 = 197.0\n",
      "Après 3850 trajectoires, nous avons G_0 = 147.0\n",
      "Après 3900 trajectoires, nous avons G_0 = 417.0\n",
      "Après 3950 trajectoires, nous avons G_0 = 18.0\n",
      "Après 4000 trajectoires, nous avons G_0 = 73.0\n",
      "Après 4050 trajectoires, nous avons G_0 = 15.0\n",
      "Après 4100 trajectoires, nous avons G_0 = 15.0\n",
      "Après 4150 trajectoires, nous avons G_0 = 375.0\n",
      "Après 4200 trajectoires, nous avons G_0 = 114.0\n",
      "Après 4250 trajectoires, nous avons G_0 = 107.0\n",
      "Après 4300 trajectoires, nous avons G_0 = 109.0\n",
      "Après 4350 trajectoires, nous avons G_0 = 89.0\n",
      "Après 4400 trajectoires, nous avons G_0 = 9.0\n",
      "Après 4450 trajectoires, nous avons G_0 = 130.0\n",
      "Après 4500 trajectoires, nous avons G_0 = 232.0\n",
      "Après 4550 trajectoires, nous avons G_0 = 500.0\n",
      "Après 4600 trajectoires, nous avons G_0 = 55.0\n",
      "Après 4650 trajectoires, nous avons G_0 = 70.0\n",
      "Après 4700 trajectoires, nous avons G_0 = 79.0\n",
      "Après 4750 trajectoires, nous avons G_0 = 19.0\n",
      "Après 4800 trajectoires, nous avons G_0 = 76.0\n",
      "Après 4850 trajectoires, nous avons G_0 = 132.0\n",
      "Après 4900 trajectoires, nous avons G_0 = 266.0\n",
      "Après 4950 trajectoires, nous avons G_0 = 156.0\n"
     ]
    }
   ],
   "source": [
    "#l'algorithme MC Control First-visit de la section 5.4 du livre de Sutton.\n",
    "def on_policy_mc(gamma=1.0):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    set_random_seed(environment, seed=23)\n",
    "\n",
    "    # On crée le Table avec n_buckets en tuple\n",
    "    table = Table((1, 1, 6, 12), environment.action_space.n)\n",
    "    \n",
    "    #Les actions possibles\n",
    "    actions=list(range(environment.action_space.n))\n",
    "    \n",
    "    epsilon = 1.0\n",
    "    \n",
    "    #Une boucle sur 5000 trajectoires\n",
    "    for i in range(5000):\n",
    "        #On stock les états, les actions et les rewards rencontrés dans notre trajectoire.\n",
    "        #La trajectoire sera donc une liste de tuple\n",
    "        trajectory = []\n",
    "        \n",
    "        #Flag pour savoir si la trajectoire est terminée\n",
    "        trajectory_done = False \n",
    "    \n",
    "        #Mise à jour de epsilon\n",
    "        epsilon=max(0.99*epsilon, 0.1)\n",
    "        \n",
    "        #On commence dans un état chosit\n",
    "        \n",
    "        s=environment.reset()\n",
    "        \n",
    "        while not trajectory_done:\n",
    "            #On discretise notre état (obtention des indexs associés\n",
    "            s_idx = discretize(s)\n",
    "            \n",
    "            #On sélectionne une action\n",
    "            a=choose_action(s_idx,table,epsilon,environment.action_space)\n",
    "            \n",
    "            #obtention du prochain état next_s, de la récompense r, et du flag, en jouant l'action choisie.\n",
    "            next_s, r, trajectory_done, _ = environment.step(a)\n",
    "            \n",
    "            #mise à jour de la trajectoire (on était dans quel état, on fait quel action et on a obtenu quel reward)\n",
    "            \n",
    "            trajectory.append((s_idx, a, r))\n",
    "            \n",
    "            #mise à jour de l'état en cours\n",
    "            s=next_s\n",
    "        \n",
    "        #Analyse de la trajectoire pour faire notre update\n",
    "        \n",
    "        G=0\n",
    "        # G sera la reward cumulée au temp t\n",
    "        \n",
    "        #On se garde une trace des états qui ont été visités (car on se mettra en mode firs-visit)\n",
    "        visited=[]\n",
    "        \n",
    "        #On parcourt à l'inverse notre trajectoire pour que nos gain cumulée veule dire quelque chose.\n",
    "        for state_idx, action, reward in reversed(trajectory):\n",
    "            G = gamma * G +reward\n",
    "            \n",
    "            # si la paire (state_idx, action) n'est pas dans une paire (state_idx, action) visité, on met\n",
    "            # à jour la valeur état action.\n",
    "            if (state_idx, action) not in visited:\n",
    "                table.store_return(state_idx, action ,G)\n",
    "                \n",
    "                visited.append((state_idx,action))\n",
    "\n",
    "        if i%50 ==0:\n",
    "            print('Après {} trajectoires, nous avons G_0 = {}'.format(i,G))\n",
    "            \n",
    "    return table\n",
    "\n",
    "mc_table = on_policy_mc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez maintenant un objet QTable qui contiendra les valeurs de Q mais mises à jour l'objectif de Q-learning.\n",
    "\n",
    "Vous pouvez réutiliser le code de Table que vous avez fait plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#6.5 of Sutton and Barto's book.\n",
    "class QTable:\n",
    "    def __init__(self, discrete_obs_shape, n_actions):\n",
    "        \"\"\"\n",
    "        Same as in Table except we do not need a ndarray for the number of visits anymore.\n",
    "        \"\"\"\n",
    "        #dimension du teableau est un tuple former de la conténation de discrete_obs_shape et n_actions\n",
    "        #Typiquement, ici la shape est (1,1,6,16,2).\n",
    "        \n",
    "        self.table_shape= discrete_obs_shape + (n_actions,)\n",
    "        \n",
    "        #Tableaux de 0 pour initialiser les valeurs de Q\n",
    "        self.Q=np.zeros(self.table_shape, dtype=np.float32)\n",
    "        \n",
    "    def update(self, obs_index, action, reward, next_obs_index, learning_rate, gamma): \n",
    "        \"\"\"\n",
    "        Updates the Q estimation for state-action pair (obs_index, action).\n",
    "\n",
    "        The update rule can be found in section 6.5 of Sutton and Barto's book.\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_Q_value = self.Q[obs_index][action]\n",
    "        increment = reward + gamma * self.Q[next_obs_index].max() - prev_Q_value #C'est l'erreur TD\n",
    "        \n",
    "        self.Q[obs_index][action] += learning_rate * increment\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez maintenant l'algorithme Q-learning de la section 6.5 du livre de Sutton et Barto en utilisant maintenant la QTable que vous venez d'implémenter.\n",
    "\n",
    "Utilisez le même régime pour epsilon que ce que vous avez utilisé pour le MC control.\n",
    "\n",
    "Utilisez learning_rate = epsilon pour chaque mise à jour.\n",
    "\n",
    "Vous devriez encore une fois être en mesure de réutiliser une bonne partie du code que vous avez fait pour le MC Control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après 0 trajectoires, nous avons G_0 = 11.0, esilon=0.99\n",
      "Après 10 trajectoires, nous avons G_0 = 20.0, esilon=0.8953382542587163\n",
      "Après 20 trajectoires, nous avons G_0 = 13.0, esilon=0.8097278682212583\n",
      "Après 30 trajectoires, nous avons G_0 = 21.0, esilon=0.7323033696543974\n",
      "Après 40 trajectoires, nous avons G_0 = 20.0, esilon=0.6622820409839835\n",
      "Après 50 trajectoires, nous avons G_0 = 13.0, esilon=0.5989560064661611\n",
      "Après 60 trajectoires, nous avons G_0 = 24.0, esilon=0.5416850759668536\n",
      "Après 70 trajectoires, nous avons G_0 = 43.0, esilon=0.4898902730042049\n",
      "Après 80 trajectoires, nous avons G_0 = 19.0, esilon=0.44304798162617254\n",
      "Après 90 trajectoires, nous avons G_0 = 23.0, esilon=0.40068465295154065\n",
      "Après 100 trajectoires, nous avons G_0 = 26.0, esilon=0.36237201786049694\n",
      "Après 110 trajectoires, nous avons G_0 = 148.0, esilon=0.3277227574378037\n",
      "Après 120 trajectoires, nous avons G_0 = 402.0, esilon=0.2963865873992079\n",
      "Après 130 trajectoires, nous avons G_0 = 500.0, esilon=0.26804671691687404\n",
      "Après 140 trajectoires, nous avons G_0 = 457.0, esilon=0.24241664604458016\n",
      "Après 150 trajectoires, nous avons G_0 = 500.0, esilon=0.21923726936647234\n",
      "Après 160 trajectoires, nous avons G_0 = 500.0, esilon=0.19827425658891445\n",
      "Après 170 trajectoires, nous avons G_0 = 99.0, esilon=0.17931568359471056\n",
      "Après 180 trajectoires, nous avons G_0 = 500.0, esilon=0.16216989001100657\n",
      "Après 190 trajectoires, nous avons G_0 = 244.0, esilon=0.1466635416321037\n",
      "Après 200 trajectoires, nous avons G_0 = 238.0, esilon=0.13263987810938213\n",
      "Après 210 trajectoires, nous avons G_0 = 500.0, esilon=0.11995712819347792\n",
      "Après 220 trajectoires, nous avons G_0 = 500.0, esilon=0.10848707650771475\n",
      "Après 230 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 240 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 250 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 260 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 270 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 280 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 290 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 300 trajectoires, nous avons G_0 = 359.0, esilon=0.1\n",
      "Après 310 trajectoires, nous avons G_0 = 22.0, esilon=0.1\n",
      "Après 320 trajectoires, nous avons G_0 = 32.0, esilon=0.1\n",
      "Après 330 trajectoires, nous avons G_0 = 189.0, esilon=0.1\n",
      "Après 340 trajectoires, nous avons G_0 = 207.0, esilon=0.1\n",
      "Après 350 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 360 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 370 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 380 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 390 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 400 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 410 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 420 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 430 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 440 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 450 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 460 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 470 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 480 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n",
      "Après 490 trajectoires, nous avons G_0 = 500.0, esilon=0.1\n"
     ]
    }
   ],
   "source": [
    "def q_learning(gamma=1.0, learning_rate=1.0):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    set_random_seed(environment, seed=42)\n",
    "\n",
    "    # On change ici qu'on crée une QTable.\n",
    "    table = QTable((1, 1, 6, 12), environment.action_space.n)\n",
    "    \n",
    "    #Les actions possibles\n",
    "    actions=list(range(environment.action_space.n))\n",
    "    \n",
    "    epsilon=1.0\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        trajectory_done = False\n",
    "        \n",
    "        #On stoquera les rewards seulement pour pouvoir imprimer notre gain à la fin\n",
    "    \n",
    "        reward=[]\n",
    "        \n",
    "        s = environment.reset()\n",
    "        \n",
    "        s_idx = discretize(s)\n",
    "        \n",
    "        while not trajectory_done:\n",
    "            a=choose_action(s_idx, table, epsilon, environment.action_space)\n",
    "            \n",
    "            next_s, r, trajectory_done, _ = environment.step(a)\n",
    "            \n",
    "            next_s_idx = discretize(next_s)\n",
    "    \n",
    "            table.update(s_idx, a, r, next_s_idx, learning_rate, gamma)\n",
    "        \n",
    "            s_idx=next_s_idx\n",
    "            \n",
    "            reward.append(r)\n",
    "    \n",
    "        epsilon=max(epsilon*0.99,0.1)\n",
    "        learning_rate=max(learning_rate*0.99,0.1)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Après {} trajectoires, nous avons G_0 = {}, esilon={}'.format(i,np.sum(reward),epsilon))\n",
    "    \n",
    "    return table\n",
    "\n",
    "q_table = q_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librairie gym vient aussi avec des outils permettant de visualiser le comportement des agents dans l'environnement.\n",
    "\n",
    "La fonction run() ci-dessous crée un wrapper autour de l'environnement et va déposer dans un dossier \"demos\" les statistiques de votre agent sur une trajectoire test sur l'environnement.\n",
    "\n",
    "Cette fonction affiche et sauvegarde aussi une vidéo montrant le déroulement en temps réel de la trajectoire dans l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(table, epsilon=0.1):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    set_random_seed(environment, seed=42)\n",
    "\n",
    "    # On ajoute un wrapper Monitor et on écrit dans un folder demos les données et la vidéo\n",
    "    env = gym.wrappers.Monitor(environment, 'demos', force=True)\n",
    "\n",
    "    done = False\n",
    "    s_idx = discretize(env.reset())\n",
    "    while not done:\n",
    "        # On rajoute un appel à render pour faire afficher les pas dans l'environnement\n",
    "        env.render()\n",
    "        action = choose_action(s_idx, table, epsilon, env.action_space)\n",
    "        next_s, r, done, _ = environment.step(action)            \n",
    "        next_s_idx = discretize(next_s)\n",
    "        s_idx = next_s_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple avec l'agent appris par Q-learning :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2f9ae047f707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-72b0defc9fca>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(table, epsilon)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0ms_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# On rajoute un appel à render pour faire afficher les pas dans l'environnement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         )\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_frames_per_sec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoder_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_path, frame_shape, frames_per_sec, output_frames_per_sec)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ffmpeg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "run(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
